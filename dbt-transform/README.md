# dbt-transform

A dbt project for transforming raw e-commerce data into clean, tested, and analytics-ready models with a focus on incremental data processing. This project takes the synthetic data generated by the DataGenerator package and creates business-ready datasets for analytics and reporting.

## Purpose

This dbt project transforms the raw data from DuckDB (generated by DataGenerator) into:
- **Clean data models**: Normalized and validated datasets
- **Business metrics**: Calculated KPIs and aggregations
- **Analytics-ready tables**: Optimized for querying and visualization
- **Incremental processing**: Efficient updates for new data without full refreshes

## Dependencies

- `dbt-core>=1.11.2`: The core dbt library for data transformation.
- `dbt-duckdb>=1.10.0`: dbt adapter for DuckDB database.

## Installation

Install the package using uv:

```bash
uv add dbt-core dbt-duckdb --package dbt-transform
```

## Project Structure

```
dbt-transform/
├── dbt_project.yml          # dbt project configuration
├── profiles.yml             # Database connection profiles
├── models/                  # dbt models (SQL transformations)
│   ├── staging/            # Raw data staging models
│   ├── intermediate/       # Intermediate transformations
│   └── marts/              # Final business-ready models
├── tests/                   # Data quality tests
├── macros/                  # Reusable SQL macros
└── docs/                    # Documentation
```

## Usage

### Initialize dbt Project
```bash
cd dbt-transform
dbt init
```

### Configure Database Connection
Update `profiles.yml` to connect to your DuckDB database:
```yaml
dbt_transform:
  target: dev
  outputs:
    dev:
      type: duckdb
      path: ../dev.duckdb
```

### Run Transformations
```bash
# Run all models
dbt run

# Run specific model
dbt run --select model_name

# Run with incremental processing
dbt run --full-refresh false
```

### Test Data Quality
```bash
# Run all tests
dbt test

# Run specific tests
dbt test --select test_name
```

### Generate Documentation
```bash
# Generate and serve docs
dbt docs generate
dbt docs serve
```

## Key Concepts

### Incremental Models
This project focuses on incremental data processing, where:
- New data is processed without reprocessing existing data
- Models use `is_incremental()` logic for conditional processing
- Efficient for large datasets with frequent updates

### Data Flow
1. **Raw Data**: Customer, order, and product data from DataGenerator
2. **Staging Models**: Initial cleaning and type casting
3. **Intermediate Models**: Business logic and calculations
4. **Mart Models**: Final analytics-ready datasets

## Development

- Models should be placed in appropriate folders (staging/intermediate/marts)
- Include data quality tests for all important business logic
- Use dbt's incremental materialization for performance
- Document all models with descriptions and column definitions